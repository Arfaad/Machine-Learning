import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LeakyReLU
from tensorflow.keras.optimizers import Adam

# Step 1: Load and preprocess the MNIST dataset
(X_train, _), (_, _) = mnist.load_data()
X_train = X_train.reshape(-1, 784).astype("float32")
X_train = (X_train - 127.5) / 127.5  # Normalize to [-1, 1]

# Latent dimension for noise vector
latent_dim = 100
img_shape = 784

# Step 2: Build the Generator
def build_generator():
    model = Sequential()
    model.add(Dense(128, input_dim=latent_dim))
    model.add(LeakyReLU(alpha=0.2))
    model.add(Dense(img_shape, activation='tanh'))
    return model

# Step 3: Build the Discriminator
def build_discriminator():
    model = Sequential()
    model.add(Dense(128, input_dim=img_shape))
    model.add(LeakyReLU(alpha=0.2))
    model.add(Dense(1, activation='sigmoid'))
    return model

# Initialize models
generator = build_generator()
discriminator = build_discriminator()

# Compile Discriminator
discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])

# Combine Generator and Discriminator (for training generator)
discriminator.trainable = False
gan = Sequential([generator, discriminator])
gan.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))

# Step 4: Function to display generated images
def sample_images(epoch, n=5):
    noise = np.random.normal(0, 1, (n * n, latent_dim))
    gen_imgs = generator.predict(noise)
    gen_imgs = 0.5 * gen_imgs + 0.5  # Rescale to [0, 1]

    fig, axs = plt.subplots(n, n, figsize=(5, 5))
    count = 0
    for i in range(n):
        for j in range(n):
            axs[i, j].imshow(gen_imgs[count].reshape(28, 28), cmap='gray')
            axs[i, j].axis('off')
            count += 1
    plt.suptitle(f"Generated Digits at Epoch {epoch}")
    plt.tight_layout()
    plt.show()

# Step 5: Train the GAN (Only 100 Epochs)
def train_gan(epochs=100, batch_size=64, sample_interval=20):
    for epoch in range(epochs):
        # Train Discriminator
        idx = np.random.randint(0, X_train.shape[0], batch_size)
        real_imgs = X_train[idx]

        noise = np.random.normal(0, 1, (batch_size, latent_dim))
        fake_imgs = generator.predict(noise)

        d_loss_real = discriminator.train_on_batch(real_imgs, np.ones((batch_size, 1)))
        d_loss_fake = discriminator.train_on_batch(fake_imgs, np.zeros((batch_size, 1)))
        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)

        # Train Generator
        noise = np.random.normal(0, 1, (batch_size, latent_dim))
        g_loss = gan.train_on_batch(noise, np.ones((batch_size, 1)))

        # Display progress
        if epoch % sample_interval == 0 or epoch == epochs - 1:
            print(f"Epoch {epoch} | D Loss: {d_loss[0]:.4f}, Acc: {100*d_loss[1]:.2f}% | G Loss: {g_loss:.4f}")
            sample_images(epoch)

# Run training for 100 epochs
train_gan(epochs=100, batch_size=64, sample_interval=20)
